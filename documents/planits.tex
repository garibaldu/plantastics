\documentclass[11pt]{article}
\input{LocalStyle.sty}  

\title{deep plantastic fabrications}
\author{marcus, will, chris}
\date{}
\begin{document}
\maketitle

\begin{abstract}
 Planning seems ludicrously hard - there must be some trick. Classical planning starts from a symbolic representation of objects, which we don't have in the wild, and performs forward and/or backward chaining to find plans. Control systems start from... stuff, and do.... stuff. Probabilistic discrete Markov chain models start from a ``1-of-N'' (punctate?) representation of state, without objects, and solve the planning problem by Value Iteration (or policy iteration I guess). Recent developments in deep learning suggest we might be able to learn rich representations of complex patterns in time, and such representations may be rather flexible. Our idea: let's think about how planning might be achieved in this kind of representation and then ask what it would take for planning to be efficiently realised in such an architecture. What would it take to make planning really easy?
\end{abstract}

\section{setup}
\begin{itemize}
 \item $\bx$, the current belief state (henceforth just ``state''). $\bx$ consists of a large number ($N$) of ``neurons'', each $x_i \in \{0,1\}$. So there are $2^N$ possible states.
 \item $\bx^\prime$, the next such state.
 \item $\bg$, the goal state, on some subset of the neurons representing belief state.
\end{itemize}

And we imagine a large number of ``action neurons'', $\ba = (a_1, a_2, \ldots, a_j,\ldots)$, with each $a_j \in \{0,1\}$


\section{what is planning}
Given transition probabilities $P(\bx^\prime \mid \bx, \ba)$ and the current $\bx$ and a goal $\bg$, the planning problem is to decide on the next action to take. In our case that means assigning binary states to all the action nodes such that the system is most likely to move to a new state from which the goal is more achievable.

The thing that sets planning apart seems to be the flexible response it can generate given a novel goal.

Thoughts:
\begin{enumerate}
 \item could do forward planning by generating potential paths, using $P(\bx^\prime \mid \bx, \ba)$, hoping one of them hits $\bg$. If it does, do the first action on that track.
 \item similarly can imagine a backward planning sampling procedure.
 \item the Right Answer is probably to write this as an MDP with appropriate rewards capturing the new goal, and then run Value Iteration to build values everywhere. But there are going to be $2^N$ states so this is silly on the face of it.
\end{enumerate}
But these will stink, for a bunch of reasons.

One is that the branching factor is astronomical.

Another is that the transition probabilities are only encoded by a bunch of weights between nodes, and these same nodes are all we have with which to represent states at any time (step in the plan).

Classical-like Planning would appear to suck.

\subsection{planning}

Suppose we have a function $f(\bx,\bg,\ba)$ that somehow measures how far we {\it expect to be from the goal} one step later, should we carry out the action $\ba$ this step.


\begin{align*}
 f(\bx,\bg,\ba) 
 &= \int_{\bx^\prime} d \bx^\prime \;\;P(\bx^\prime \mid \bx, \ba) \;\,  \norm{ \bg - \bx^\prime }^\text{plan}
\end{align*}
where the superscript ``plan'' indicates that the ``norm'' we care about is the number of steps in the subsequent plan. Knowing that, or having a good guess, ought to make the process of deciding actions easier, which is why we suggest it could be ``built in'' via the representation of $\bx$ itself, if that's possible.

(Q: what heuristic / approximation would still be good enough for $f$? Or does it have to be exact not to exhibit toxic outcomes? What's the connection, if any, to the ``admissible'' heuristics (that don't over-estimate) of $A^\star$ search?)

\section{accessibility as topology}

SUPPOSE we have a bit-string representation of state such that, for each state $\bx$ {\bf all the successor states $\bx^\prime$ accessible by one action are one bit away} from $\bx$. We'll probably also need the converse: no state that is inaccessible in one action from $\bx$ is only one bit different from it.

We suspect this should make planning efficient.

Notice that $P(\bx^\prime \mid \bx, \ba)$ in its full generality is going to be a distribution over all $2^N$ bit-strings.
But our assumption means that all these are zero except those 1 bit away from $\bx$, so it's really a normalised distribution over just $N$ strings. 

Since our assumption means we can replace the plan ``norm'' by the 1-norm...
\begin{align*}
 f(\bx,\bg,\ba) &= \int_{\bx^\prime} d \bx^\prime \;\;
 P(\bx^\prime \mid \bx, \ba) \;\,  \norm{ \bg - \bx^\prime }^1
\end{align*}

Suppose that for each $i$ we have a prediction $ y_i = \Pr(x_i^\prime =1 \mid \bx, \ba)$. Then I think we can say
\begin{align*}
f(\bx,\bg,\ba)
&=  \sum_{x_1^\prime=0}^1 \ldots \sum_{x_j^\prime=0}^1 \ldots \sum_{x_n^\prime=0}^1 P(\bx^\prime \mid \bx, \ba) \;\,  \norm{ \bg - \bx^\prime }^1 
\intertext{replacing the integral with sum over vertices of hypercube}
&= \sum_{x_1^\prime=0}^1 \ldots \sum_{x_j^\prime=0}^1 \ldots \sum_{x_n^\prime=0}^1  \; \bigg( \prod_{k} P(x_{k}^\prime \mid \bx, \ba) \bigg) \;\;  \norm{ \bg - \bx^\prime}^1 \\
&= \sum_{x_1^\prime=0}^1 P(x_{1}^\prime \mid \bx, \ba) \ldots \sum_{x_j^\prime=0}^1 P(x_{j}^\prime \mid \bx, \ba) \ldots \sum_{x_n^\prime=0}^1  P(x_{n}^\prime \mid \bx, \ba)
\;\;  \norm{ \bg - \bx^\prime }^1 \\
 &= \sum_i \norm{ g_i - y_i } \;\;{\color{blue}\text{I think!! Anyone care to prove this?}}\\
 &= \sum_i
  \begin{cases}
    1-y_i, & \text{if $g_i=1$}.\\
    y_i, & \text{if $g_i=0$}.
  \end{cases} \intertext{which for example we can write as}
  &= - \sum_i \bigg[ G_i y_i \, - \, g_i \bigg]
\end{align*}
where $G_i$ is the ``signed'' version of the goal, $G_i = 2g_i-1$.
So which action $a_j \in \{0,1\}$ should the $j^\text{th}$ action neuron adopt? 
To answer we need to decide on a form for the prediction $y_i$. 

\subsection{consequences for the simplest predictions}
The most obvious simple form would seem to be a squashing function applied to a weighted sum,
\begin{align*}
 y_i &= \frac{1}{1+e^{-\phi}}
\intertext{to ensure the range $\left[ 0,1 \right]$, with $\phi$ being a weighted sum over stuff that matters:}
 \phi_i &= \sum_j W_{ij} \, a_j \; + \; \sum_{k} U_{ik} \, x_k \; + \; B_i
\end{align*}
where $W$ are weights from action neurons to state neurons, $U$ are weights from state neurons to one another, and $B$ are bias weights.

Looking at the direct difference between $f(\bx,\bg,\ba)$ values in the two cases gets ugly as it's the difference between two sigmoids. That's awkward.
Perhaps we can look at the gradient instead:
\begin{align*}
\swrt{f}{a_j} &= - \, \sum_i G_i \; y_i (1-y_i) \; W_{ij}
\end{align*}
We should make $a_j=1$ if this is negative, which suggests actions should be chosen like a ``perceptron'' (linear threshold) whose input is a weighted sum of ``inputs'' 
$G_i y_i (1-y_i)$ and whose weights are exactly the predictive ones, run in their reverse direction. That's pretty interesting and potentially a very fast calculation for an agent to make.

However what actions for all the action neurons (including the $j^\text{th}$ itself) should we use in setting the $\phi$ in this equation? It's not obvious to me.

They're all interacting in the production of a prediction. So although we can take some given $\ba$ and ask ``should the $j^\text{th}$ neuron switch states?'', it's not clear how $\ba$ came about. A long MCMC-style process of discovery for $\ba$ seems impractical for any time-stressed agent!  
THOUGHTS?



% \subsection{maybe this is an equivalent view}
% I think the sufficient statistics of that distribution can be captured by its ``center of mass'', $\bm^\prime = \expectation \left[ \bx^\prime \right]$, where the expectation is over $P(\bx^\prime \mid \bx, \ba)$. 
% 
% $\bm^\prime$ is a point somewhere in a volume defined by the union of $\bx$ and those vertices on the hypercube 1 bit away\footnote{Equivalently, on a simplex defined by the union of $\bx$ and those vertices on the hypercube 1 bit away.}. It's merely a point in $\reals^N$, meaning we're working with just $N$ degrees of freedom instead of $2^N$.
% 
% Suppose we have a function $f(\bx,\bg,\ba)$ that decreases monotonically with the difference between $\bm^\prime (\bx,\ba)$ and $\bg$.
% 
% A helpful notation is $\ba_{\wo j}$, which means the activities of all the action neurons {\it apart from the $j^\text{th}$}.
% 
% Then ``planning'', in terms of say the $j^\text{th}$ action neuron, seems to reduce to choosing  $a_j$ to be 0 or 1 based on whether 
% \[ 
%  f(\bx, \bg, \ba_{\wo j}, a_j=1) >  f(\bx, \bg, \ba_{\wo j}, a_j=0)
% \]
% 
% We might denote the difference by 
% \[\Delta_j f(\bx, \bg, \ba_{\wo j})\]
% 
% The action should be 1 if this is positive and zero otherwise.
% 
% To decide $a_j$ quickly, \underline{ the calculation of $\Delta_j f(\bx, \bg, \ba_{\wo j}) $ has to be {\it really easy}}. Right?
% 
%  \subsection{}
% 
% 
% \subsection{a WRONG candidate for $f$}
% I suggest the KL divergence,
% \begin{align*}
%  f(\bx, \bg, \ba) &= \sum_{i=1}^N g_i \log y_i \; + \; (1-g_i) \log (1-y_i)
%  \intertext{where}
%  y_i &= \Pr(x_i^\prime =1 \mid \bx, \ba)
% \intertext{
% This means the difference between the two $f$ values for action node $j$ is}
%  \Delta_j f(\bx, \bg, \ba_{\wo j}) &= \sum_{i=1}^N \bigg[ g_i \log \frac{y_i^1}{y_i^0} \; + \; (1-g_i) \log \frac{(1-y_i^1)}{(1-y_i^0)} \bigg]
%  \intertext{where}
% y_i^{A_j} &= \Pr(x_i^\prime =1 \mid \bx, \ba_{\wo j}, a_j=A_j) 
% \end{align*}
% 
% 
% 
% So looking at the first term for example, $\log \frac{y_i^1}{y_i^0}$ needs to be easy. 
% 
% This seems spectacularly unpromising, and might mean some other candidate for $f$ should be sought! 
% 
% On the other hand, if we do the obvious thing for $y$ and use a sigmoid function of weighted input $\phi$ from both $\ba$ and $\bx$, then the first term involves
% \begin{align*}
% \log \frac{y_i^1}{y_i^0} &= 
% \log (1+e^{-\phi_{\wo j}}) - \log (1+e^{-\phi_{\wo j} \, -w_{ij}})
% \end{align*}
% So those are ``hinge'' functions: basically they're zero if the exponent is negative, and match the exponent if it's positive.
% 
% What does this mean?!
% 
% I suspect there are several cases but it's going to be close to being just $w_{ij}$ in some of them, namely the learned weight from the $j^\text{th}$ action node into the $i^\text{th}$ state node. And THAT would lead to the $a_j$ decision having something close to a weighted sum of inputs {\it from } state nodes {\it in their goal state}.... Of course, I have only done the first term in $\Delta f$ here, not the second. But it's suggestive. The second term might result in almost the same thing but with negatives on the weights when the goal $g_i=0$. 
% 
% So in my dreams I see us ``deriving'' the action choice $a_j=1$ with a probability given by sigmoid function of a {\it weighted sum of inputs into the action neuron} where 
% \begin{itemize}
%  \item the weights are the same as those going in the opposite direction, namely the predictive weights $w_{ij}$ from the action neuron $j$ into the belief-state neuron $i$;
%  \item the ``inputs'' are 1 if $g_i=1$ and -1 if $g_i=0$
% \end{itemize}
% That'd be about as fast as you could expect then!
% 
% It's not going to be quite that simple though - those hinge functions are complicated... There's some sort of ``veto'' being played out... Notice the (horrible) interaction between {\bf all} the action neurons, coming through the exponentiation of $\phi_{\wo j}$. What a pest. Hard to imagine that going away unless I stop thinking of $\ba$ as causal regarding state changes. An RBM between $a$ and $x$ instead...? What would that even mean?!
% 
% \subsection{Next steps}
% \begin{itemize}
% \item think about that last sentence in my sleep.
% \item Figure out the hinge extremal cases to get a feel.
% \item In those extremal cases, does this actually lead to action decisions that look sane?
% \item SHOULD look for another $f$ choice and see if it's more straightforward, e.g. squared errors. My gut says it won't be.
% \end{itemize}
% 

\end{document}
